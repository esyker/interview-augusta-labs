{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected processing time: 0 minutes and 2 seconds.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class WikipediaSearch:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def list_last_pt_articles(self, total_limit: int = 500) -> List[Dict]:\n",
    "        \"\"\"Lists the last articles.\"\"\"\n",
    "        search_results = []\n",
    "        url = f\"https://pt.wikipedia.org/w/index.php?title=Especial:P%C3%A1ginas_novas&limit={total_limit}\"\n",
    "        \n",
    "        # Make a single request to get the list of new pages\n",
    "        res = requests.get(url)\n",
    "        \n",
    "        if res.status_code == 200:\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(res.content, 'html.parser')\n",
    "            \n",
    "            # Find all <ul> elements with class 'mw-contributions-list'\n",
    "            ul_elements_list = soup.find_all('ul', class_='mw-contributions-list')\n",
    "            for ul_element in ul_elements_list:\n",
    "                # Get all the list items (li) inside the ul\n",
    "                list_items = ul_element.find_all('li')\n",
    "                \n",
    "                for item in list_items:\n",
    "                    # Extract article date and hyperlink\n",
    "                    item_hyperlink = item.find('a')\n",
    "                    article_date = item_hyperlink.text.strip() if item_hyperlink else None\n",
    "                    article_link = f\"https://pt.wikipedia.org{item_hyperlink['href']}\" if item_hyperlink else None\n",
    "                    search_results.append({'date': article_date, 'link': article_link})\n",
    "        else:\n",
    "            raise ValueError(f\"Failed to retrieve data. Status code: {res.status_code}\")\n",
    "        \n",
    "        return search_results\n",
    "\n",
    "    def parse_article(self, article: Dict, processing_type: str = 'default', parse_text_processing_type: str = 'simple') -> Dict:\n",
    "        \"\"\"Fetches the article content from its link and adds the text to the article dict.\n",
    "        \n",
    "        Args:\n",
    "            article (Dict): The article dictionary containing the link.\n",
    "            processing_type (str): The type of processing to use for extracting text.\n",
    "                                   Options: 'default' (parses HTML) or 'simple' (gets text directly).\n",
    "            parse_text_processing_type (str): The type of text processing to apply after fetching the article.\n",
    "                                               Options: 'default' (processes as HTML) or 'simple' (raw text).\n",
    "\n",
    "        Returns:\n",
    "            Dict: The updated article dictionary with the text added.\n",
    "        \"\"\"\n",
    "        article_url = article.get(\"link\")\n",
    "        if not article_url:\n",
    "            return article\n",
    "        \n",
    "        res = requests.get(article_url)\n",
    "        \n",
    "        if res.status_code == 200:\n",
    "            if processing_type == 'default':\n",
    "                soup = BeautifulSoup(res.content, 'html.parser')\n",
    "                \n",
    "                # Parse the content from the main body of the article\n",
    "                content = soup.find('div', class_='mw-parser-output')\n",
    "                if content:\n",
    "                    paragraphs = content.find_all('p')\n",
    "                    # Join all paragraph texts to form the full article text\n",
    "                    article_text = '\\n'.join([para.get_text(strip=False) for para in paragraphs])\n",
    "                    article[\"text\"] = article_text\n",
    "                else:\n",
    "                    article[\"text\"] = \"No content available\"\n",
    "            elif processing_type == 'simple':\n",
    "                # Simple processing type: directly extract text from the response\n",
    "                article[\"text\"] = res.text\n",
    "            else:\n",
    "                raise ValueError(\"Invalid processing type specified.\")\n",
    "            \n",
    "            # Apply text processing type\n",
    "            if parse_text_processing_type == 'simple':\n",
    "                # Directly extract text (no HTML processing)\n",
    "                article[\"text\"] = res.text\n",
    "            elif parse_text_processing_type != 'default':\n",
    "                article[\"text\"] += \" Invalid text processing type specified.\"\n",
    "        else:\n",
    "            article[\"text\"] = \"Failed to fetch article content\"\n",
    "        \n",
    "        return article\n",
    "\n",
    "    def calculate_processing_time(self, num_articles: int, requests_per_second: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the expected time to process a given number of articles at a specified rate of requests per second.\n",
    "        \"\"\"        \n",
    "        # Total time to process all articles\n",
    "        total_time = num_articles / requests_per_second\n",
    "        return total_time\n",
    "\n",
    "    def get_last_pt_articles(self, total_limit: int = 10, requests_per_second: int = 10000,\n",
    "                            parse_text_processing_type = \"simple\", \n",
    "                            verbose=True) -> List[Dict]:\n",
    "        \"\"\"Combines listing and parsing of the articles, applying a request rate limit.\"\"\" \n",
    "        articles = self.list_last_pt_articles(total_limit=total_limit)\n",
    "        parsed_articles = []\n",
    "\n",
    "        # Calculate the delay between requests to achieve the desired rate\n",
    "        delay = 1 / requests_per_second  # Time to wait between requests\n",
    "\n",
    "        # Use tqdm with total parameter for correct progress display\n",
    "        for article in tqdm(articles, total=len(articles) if verbose else None):\n",
    "            # Parse the article's content\n",
    "            parsed_article = self.parse_article(article, parse_text_processing_type = parse_text_processing_type)\n",
    "            parsed_articles.append(parsed_article)\n",
    "\n",
    "            # Apply rate limiting: wait before processing the next article\n",
    "            time.sleep(delay)\n",
    "\n",
    "        return parsed_articles\n",
    "\n",
    "# Usage example\n",
    "wiki_search = WikipediaSearch()\n",
    "total_limit = 10\n",
    "requests_per_second = 4\n",
    "parse_text_processing_type = \"default\"\n",
    "verbose = True\n",
    "expected_processing_time = wiki_search.calculate_processing_time(num_articles=total_limit, requests_per_second=requests_per_second)\n",
    "print(f\"Expected processing time: {int(expected_processing_time // 60)} minutes and {int(expected_processing_time % 60)} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 10 articles fetched and parsed.\n",
      "Date: 16h43min de 5 de outubro de 2024, Link: https://pt.wikipedia.org/w/index.php?title=Glucano_1,4-alfa-glicosidase&oldid=68762060, Text: A glucana 1,4-α-glicosidase' (Numero EC: 3.2.1.3), anteriormente conhecida por γ-amilase', é uma enz...\n",
      "Date: 16h10min de 5 de outubro de 2024, Link: https://pt.wikipedia.org/w/index.php?title=Rep%C3%BAblica_de_Labin&oldid=68761894, Text: Labinska Republika (Croata)Repubblica di Albona (Italiano)\n",
      "Estado não reconhecido\n",
      "A República de Lab...\n",
      "Date: 16h10min de 5 de outubro de 2024, Link: https://pt.wikipedia.org/w/index.php?title=Ruyter_Poubel&oldid=68761891, Text: Ruyter de Mendonça Poubel (Duque de Caxias, 16 de julho de 1997), mais conhecido como Ruyter Poubel,...\n",
      "Date: 16h04min de 5 de outubro de 2024, Link: https://pt.wikipedia.org/w/index.php?title=Ansalonga&oldid=68761865, Text: Ansalonga é uma localidade de Andorra, situada na Paróquia de Ordino. Situada a 1.331 metros de alti...\n",
      "Date: 16h00min de 5 de outubro de 2024, Link: https://pt.wikipedia.org/w/index.php?title=Nick_Saban&oldid=68761846, Text: Nicholas Lou Saban Jr. (Fairmont, 31 de outubro de 1951) é um comentarista esportivo e ex-treinador ...\n",
      "Date: 15h46min de 5 de outubro de 2024, Link: https://pt.wikipedia.org/w/index.php?title=FACEIT_Major:_London_2018&oldid=68761766, Text: O FACEIT Major: London 2018, também conhecido como FACEIT Major 2018, ou London 2018, foi o décimo t...\n",
      "Date: 15h34min de 5 de outubro de 2024, Link: https://pt.wikipedia.org/w/index.php?title=Conquista_do_Alentejo&oldid=68761723, Text:  Geraldo Sem-Pavor\n",
      " Gonçalo Mendes da Maia\n",
      " D. Soeiro Viegas\n",
      "\n",
      "Conflitos do século IX\n",
      "\n",
      "Conflitos do s...\n",
      "Date: 15h17min de 5 de outubro de 2024, Link: https://pt.wikipedia.org/w/index.php?title=Deslocamento_liban%C3%AAs_durante_a_Guerra_Israel%E2%80%93Hezbollah&oldid=68761667, Text: O deslocamento libanês durante a Guerra Israel–Hezbollah refere-se aos cidadãos e residentes permane...\n",
      "Date: 15h12min de 5 de outubro de 2024, Link: https://pt.wikipedia.org/w/index.php?title=Filho_do_C%C3%A9u&oldid=68761635, Text: Filho do Céu, ou Tianzi (em chinês: 天子; pinyin: Tiānzǐ), era o título sagrado monárquico e imperial ...\n",
      "Date: 14h29min de 5 de outubro de 2024, Link: https://pt.wikipedia.org/w/index.php?title=Time_Cut&oldid=68761473, Text: Time Cut (bra: Corte no Tempo) é um futuro filme de americano dos gêneros ficção científica e terror...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "articles = wiki_search.get_last_pt_articles(total_limit=total_limit, requests_per_second=requests_per_second,\n",
    "                    parse_text_processing_type = parse_text_processing_type, verbose = verbose)\n",
    "print(f\"Total of {len(articles)} articles fetched and parsed.\")\n",
    "for article in articles:\n",
    "    print(f\"Date: {article['date']}, Link: {article['link']}, Text: {article.get('text', 'No content')[:100]}...\")  # Print first 100 chars of the article text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
